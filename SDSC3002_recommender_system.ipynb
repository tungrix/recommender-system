{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VeIUjLjOtVGIgsOSRFC3sTg7ayGGnpES",
      "authorship_tag": "ABX9TyOFDpRocLbWY2iiL13hIWua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tungrix/recommender-system/blob/Joel/SDSC3002_recommender_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17OmgavQfp4"
      },
      "source": [
        "# Recommending movies: retrieval with distribution strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "In this tutorial, we're going to train the same retrieval model as we did in the [basic retrieval](basic_retrieval) tutorial, but with distribution strategy.\n",
        "\n",
        "We're going to:\n",
        "\n",
        "1. Get our data and split it into a training and test set.\n",
        "2. Set up two virtual GPUs and TensorFlow MirroredStrategy.\n",
        "3. Implement a retrieval model using MirroredStrategy.\n",
        "4. Fit it with MirrorredStrategy and evaluate it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawo1x8kQk9b"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0vJOdh9WbTpd",
        "outputId": "8829cdf6-4ff3-4f75-cb49-ec7292167e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BxQ_hy7xPH3N"
      },
      "outputs": [],
      "source": [
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We prepare the dataset in exactly the same way as we do in the [basic retrieval](basic_retrieval) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "outputId": "b2f39978-9e3d-4814-c89a-4b9e27f0e02f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvKO86j9lZw7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bucketized_user_age': 45.0,\n",
            " 'movie_genres': array([7]),\n",
            " 'movie_id': b'357',\n",
            " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
            " 'raw_user_age': 46.0,\n",
            " 'timestamp': 879024327,\n",
            " 'user_gender': True,\n",
            " 'user_id': b'138',\n",
            " 'user_occupation_label': 4,\n",
            " 'user_occupation_text': b'doctor',\n",
            " 'user_rating': 4.0,\n",
            " 'user_zip_code': b'53211'}\n",
            "{'movie_genres': array([4]),\n",
            " 'movie_id': b'1681',\n",
            " 'movie_title': b'You So Crazy (1994)'}\n"
          ]
        }
      ],
      "source": [
        "# Ratings data.\n",
        "ratings_raw = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies_raw = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "for x in ratings_raw.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)\n",
        "\n",
        "for x in movies_raw.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in ratings_raw.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x[\"movie_genres\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u3duuN_Lkvj",
        "outputId": "baf61d7e-1b56-478d-a951-84afe869f5d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array([7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the overlapping nature of some of the features in the dataset (see `'bucketized_user_age'`/`'raw_user_age'`, `'user_occupation_label'`/`'user_occupation_text'`)"
      ],
      "metadata": {
        "id": "ZtZKlZweJ-WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings_raw.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "    \"user_occupation_label\": tf.strings.as_string(x[\"user_occupation_label\"]),\n",
        "    \"bucketized_user_age\": tf.strings.as_string(tf.dtypes.cast(x[\"bucketized_user_age\"], tf.int32)),\n",
        "    \"user_gender\": tf.strings.as_string(x[\"user_gender\"]),\n",
        "    \"timestamp\": x[\"timestamp\"]\n",
        "})\n",
        "movies = movies_raw.map(lambda x: x[\"movie_title\"])\n",
        "\n",
        "movie_titles = movies.batch(100_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])"
      ],
      "metadata": {
        "id": "53b8O8ZdwnP2"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ratings = ratings.batch(1_000_000).map(lambda x: x[\"user_rating\"])\n",
        "unique_ratings = np.unique(np.concatenate(list(user_ratings)))"
      ],
      "metadata": {
        "id": "zBl3Nv7ivzki"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_occupations = ratings.batch(1_000_000).map(lambda x: x[\"user_occupation_label\"])\n",
        "unique_occupations = np.unique(np.concatenate(list(user_occupations)))"
      ],
      "metadata": {
        "id": "jjbdmUlcwiBy"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_occupations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEFkneogIoK3",
        "outputId": "d90a6f98-796e-4696-89c2-a96db7c6b145"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'0', b'1', b'10', b'11', b'12', b'13', b'14', b'15', b'17', b'18',\n",
              "       b'2', b'21', b'4', b'5', b'6', b'8', b'9'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_ages = ratings.batch(1_000_000).map(lambda x: x[\"bucketized_user_age\"])\n",
        "unique_ages = np.unique(np.concatenate(list(user_ages)))"
      ],
      "metadata": {
        "id": "iCZBHZ9ExFGl"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_ages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARqdTJSlIsYE",
        "outputId": "d7344864-47d5-498b-c426-f6ba6b2322d0"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'1', b'18', b'25', b'35', b'45', b'50', b'56'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_genders = ratings.batch(1_000_000).map(lambda x: x[\"user_gender\"])\n",
        "unique_genders = np.unique(np.concatenate(list(user_genders)))"
      ],
      "metadata": {
        "id": "lhuolFf4xtoK"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "unique_movie_titles[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0B8rrVdZvsx",
        "outputId": "40365427-46b4-4424-d852-5fbe3795ccb5"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
              "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
              "       b'2 Days in the Valley (1996)',\n",
              "       b'20,000 Leagues Under the Sea (1954)',\n",
              "       b'2001: A Space Odyssey (1968)',\n",
              "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
              "       b'39 Steps, The (1935)'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "QVkhvySn8TSo"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCaRIoAVlZw7"
      },
      "source": [
        "## Set up two virtual GPUs\n",
        "\n",
        "If you have not added GPU accelerators to your Colab, please disconnect the Colab runtime and do it now. We need the GPU to run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_FxAnj40lZw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f3d381-e6bd-4a04-a623-4674ac2e50f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual devices cannot be modified after being initialized\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "  # Create 2 virtual GPUs with 1GB memory each\n",
        "  try:\n",
        "    tf.config.set_logical_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
        "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
        "    logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
        "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8HyLUirlZw7"
      },
      "source": [
        "## Implementing a model\n",
        "\n",
        "We implement the user_model, movie_model, metrics and task in the same way as we do in the [basic retrieval](basic_retrieval) tutorial, but we wrap them in the distribution strategy scope:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 32"
      ],
      "metadata": {
        "id": "hq0EpACI2MF4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_user_ids, mask_token=None),\n",
        "        # We add an additional embedding to account for unknown tokens.\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # self.user_occupation = tf.keras.Sequential([\n",
        "    #     tf.keras.layers.StringLookup(\n",
        "    #       vocabulary=unique_occupations, mask_token=None),\n",
        "    #     # We add an additional embedding to account for unknown tokens.\n",
        "    #     tf.keras.layers.Embedding(len(unique_occupations) + 1, embedding_dimension)\n",
        "    # ])\n",
        "\n",
        "    self.user_age = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_ages, mask_token=None),\n",
        "        # We add an additional embedding to account for unknown tokens.\n",
        "        tf.keras.layers.Embedding(len(unique_ages) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # self.user_gender = tf.keras.Sequential([\n",
        "    #     tf.keras.layers.StringLookup(\n",
        "    #       vocabulary=unique_genders, mask_token=None),\n",
        "    #     # We add an additional embedding to account for unknown tokens.\n",
        "    #     tf.keras.layers.Embedding(len(unique_genders) + 1, embedding_dimension)\n",
        "    # ])\n",
        "\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, embedding_dimension),\n",
        "    ])\n",
        "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "\n",
        "    self.normalized_timestamp.adapt(timestamps)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        # self.user_occupation(inputs[\"user_occupation_label\"]),\n",
        "        self.user_age(inputs[\"bucketized_user_age\"]),\n",
        "        # self.user_gender(inputs[\"user_gender\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1))\n",
        "    ], axis=1)"
      ],
      "metadata": {
        "id": "_0Sc9u-sYdK6"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=max_tokens)\n",
        "    \n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      self.title_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, embedding_dimension, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer.adapt(movies)\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(input),\n",
        "        self.title_text_embedding(input),\n",
        "    ], axis=1)\n"
      ],
      "metadata": {
        "id": "pV6339iHYmYP"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "olMkFgaVlZw7"
      },
      "outputs": [],
      "source": [
        "embedding_dimension = 32\n",
        "\n",
        "with strategy.scope():\n",
        "    query_model = tf.keras.Sequential([\n",
        "      UserModel(),\n",
        "      tf.keras.layers.Dense(64)\n",
        "    ])\n",
        "    candidate_model = tf.keras.Sequential([\n",
        "      MovieModel(),\n",
        "      tf.keras.layers.Dense(64)\n",
        "    ])\n",
        "    task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(candidate_model),\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CfedlMhlZw7"
      },
      "source": [
        "We can now put it all together into a model. This is exactly the same as in the [basic retrieval](basic_retrieval) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.query_model = query_model\n",
        "    self.candidate_model = candidate_model\n",
        "    self.task = task\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        # \"user_occupation_label\": features[\"user_occupation_label\"],\n",
        "        \"bucketized_user_age\": features[\"bucketized_user_age\"],\n",
        "        # \"user_gender\": features[\"user_gender\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "    })\n",
        "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(query_embeddings, movie_embeddings)"
      ],
      "metadata": {
        "id": "YG7Ryuog7DvA"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3meh377lZw8"
      },
      "source": [
        "## Fitting and evaluating\n",
        "\n",
        "Now we instantiate and compile the model within the distribution strategy scope.\n",
        "\n",
        "Note that we are using Adam optimizer here instead of Adagrad as in the [basic retrieval](basic_ranking) tutorial since Adagrad is not supported here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "9HU0HFLKlZw8"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  model = MovielensModel()\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1, weight_decay=0.001)) # optimizer=tf.keras.optimizers.Adagrad(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fSDiMmilZw8"
      },
      "source": [
        "Then shuffle, batch, and cache the training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "TGi_aZL0lZw8"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WraDOk09lZw8"
      },
      "source": [
        "Then train the  model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnnOau1xlZw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204a194e-ce9e-49c1-a9a8-cce7d6dfb2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'cond/Identity_4:0' shape=(None,) dtype=string>, 'bucketized_user_age': <tf.Tensor 'cond/Identity:0' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'cond/Identity_2:0' shape=(None,) dtype=int64>}. Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'cond/Identity_4:0' shape=(None,) dtype=string>, 'bucketized_user_age': <tf.Tensor 'cond/Identity:0' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'cond/Identity_2:0' shape=(None,) dtype=int64>}. Consider rewriting this model with the Functional API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 21s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0476 - factorized_top_k/top_5_categorical_accuracy: 0.0580 - factorized_top_k/top_10_categorical_accuracy: 0.0654 - factorized_top_k/top_50_categorical_accuracy: 0.0960 - factorized_top_k/top_100_categorical_accuracy: 0.1237 - loss: 82793.2592 - regularization_loss: 0.0000e+00 - total_loss: 82793.2592\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 1.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0026 - factorized_top_k/top_10_categorical_accuracy: 0.0059 - factorized_top_k/top_50_categorical_accuracy: 0.0372 - factorized_top_k/top_100_categorical_accuracy: 0.0798 - loss: 69746.9474 - regularization_loss: 0.0000e+00 - total_loss: 69746.9474\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 14s 1s/step - factorized_top_k/top_1_categorical_accuracy: 2.2500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0036 - factorized_top_k/top_10_categorical_accuracy: 0.0084 - factorized_top_k/top_50_categorical_accuracy: 0.0568 - factorized_top_k/top_100_categorical_accuracy: 0.1260 - loss: 68344.8707 - regularization_loss: 0.0000e+00 - total_loss: 68344.8707\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 12s 1s/step - factorized_top_k/top_1_categorical_accuracy: 3.6250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0047 - factorized_top_k/top_10_categorical_accuracy: 0.0108 - factorized_top_k/top_50_categorical_accuracy: 0.0749 - factorized_top_k/top_100_categorical_accuracy: 0.1642 - loss: 67259.4865 - regularization_loss: 0.0000e+00 - total_loss: 67259.4865\n",
            "Epoch 5/10\n",
            " 1/10 [==>...........................] - ETA: 9s - factorized_top_k/top_1_categorical_accuracy: 3.6621e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0051 - factorized_top_k/top_10_categorical_accuracy: 0.0131 - factorized_top_k/top_50_categorical_accuracy: 0.0839 - factorized_top_k/top_100_categorical_accuracy: 0.1862 - loss: 70129.3984 - regularization_loss: 0.0000e+00 - total_loss: 70129.3984"
          ]
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = model.evaluate(\n",
        "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "\n",
        "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
      ],
      "metadata": {
        "id": "L8EinIdYfR1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = model.evaluate(\n",
        "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "\n",
        "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvkNJaesbTNi",
        "outputId": "0afb7661-368e-461c-faa3-09c0f1a962ca"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'cond/Identity_4:0' shape=(None,) dtype=string>, 'user_occupation_label': <tf.Tensor 'cond/Identity_5:0' shape=(None,) dtype=string>, 'bucketized_user_age': <tf.Tensor 'cond/Identity:0' shape=(None,) dtype=string>, 'user_gender': <tf.Tensor 'cond/Identity_3:0' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'cond/Identity_2:0' shape=(None,) dtype=int64>}. Consider rewriting this model with the Functional API.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 10s 962ms/step - factorized_top_k/top_1_categorical_accuracy: 3.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0029 - factorized_top_k/top_10_categorical_accuracy: 0.0085 - factorized_top_k/top_50_categorical_accuracy: 0.1061 - factorized_top_k/top_100_categorical_accuracy: 0.2380 - loss: 32416.6673 - regularization_loss: 0.0000e+00 - total_loss: 32416.6673\n",
            "Top-100 accuracy (test): 0.24.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3pr3X8ulZw8"
      },
      "source": [
        "You can see from the training log that TFRS is making use of both virtual GPUs.\n",
        "\n",
        "Finally, we can evaluate our model on the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVKGDqZKlZw8"
      },
      "source": [
        "This concludes the retrieval with distribution strategy tutorial."
      ]
    }
  ]
}